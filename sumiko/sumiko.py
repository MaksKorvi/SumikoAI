import os
import logging
import torch
import asyncio
import pandas as pd
from dotenv import load_dotenv
from typing import List
from telegram import Update
from telegram.ext import Application, MessageHandler, CommandHandler, filters, ContextTypes
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import Dataset
from peft import LoraConfig, get_peft_model, TaskType

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
MODEL_NAME = "sberbank-ai/rugpt3medium_based_on_gpt2"
MAX_RESPONSE_LENGTH = 200
TRAIN_STEPS = 5
BAD_WORDS = ["–±–ª—è—Ç—å", "—Å—É–∫–∞", "–µ–±–ª–∞–Ω", "—Ö—É–π", "–∑–∞–ª—É–ø–∞", "—É–µ–±–∏—â–µ", "–¥—É—Ä–∞", "—à–ª—é—Ö–∞", "—É–±–µ–∂–∏—â–µ", "–∞—Ö—É–µ–ª–∞", "–º—Ä–∞–∑—å", "—Ç–≤–∞—Ä—å", "–º—Ä–∞–∑–æ—Ç–∞", "–ø–∏–¥–æ—Ä", "–µ–±–∞–Ω—É—Ç–∞—è", "–∂–∏–≤–æ—Ç–Ω–æ–µ"]  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞

SYSTEM_PROMPT = """..."""  # –í—Å—Ç–∞–≤—å—Ç–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
special_tokens = ['<SUMI>', '</SUMI>'] + ['‚ô™', '(‚âßœâ‚â¶)', '(‚óï‚Äø‚óï)', '(=^ÔΩ•œâÔΩ•^=)', '‚ô°']
tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
model.resize_token_embeddings(len(tokenizer))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def contains_profanity(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—É—é –ª–µ–∫—Å–∏–∫—É"""
    text_lower = text.lower()
    return any(bad_word in text_lower for bad_word in BAD_WORDS)

def filter_profanity(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤"""
    words = text.split()
    cleaned_words = [word if not contains_profanity(word) else "*—Ü–µ–Ω–∑—É—Ä–∞*" for word in words]
    return " ".join(cleaned_words)

def format_prompt(user_input, history=None):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π"""
    system_part = f"<SUMI>{SYSTEM_PROMPT}</SUMI>\n"
    history_part = ""
    if history:
        for u, r in history[-3:]:
            history_part += f"üë§: {u}\nüê±: {r}\n"
    return f"{system_part}{history_part}üë§: {user_input}\nüê±:"

def generate_response(text, chat_history=None):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ü–µ–Ω–∑—É—Ä—ã"""
    if contains_profanity(text):
        return "–ú–Ω–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—Ä–µ—Ç–∏–ª –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ç–∞–∫–∏–µ —Å–ª–æ–≤–∞! (‚óïÔ∏µ‚óï)"
    
    prompt = format_prompt(text, chat_history)
    inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).to(device)
    
    outputs = model.generate(
        inputs.input_ids,
        max_length=min(inputs.input_ids.shape[1] + MAX_RESPONSE_LENGTH, 1024),
        temperature=0.9,
        top_k=40,
        top_p=0.92,
        repetition_penalty=1.15,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )
    
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return postprocess_response(response)

def postprocess_response(text):
    """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
    text = text.split("üë§:")[0].strip()
    
    if contains_profanity(text):
        return "–û–π, —è –Ω–µ –º–æ–≥—É —Ç–∞–∫–æ–µ —Å–∫–∞–∑–∞—Ç—å! (=ÔΩÄœâ¬¥=)"
    
    if not any(e in text for e in ['‚ô™', '‚âßœâ‚â¶', '‚óï‚Äø‚óï']):
        text += random.choice([' (‚âßœâ‚â¶)', ' (=^ÔΩ•œâÔΩ•^=)', ' ‚ô™'])
    
    return apply_cuteness(text[:MAX_RESPONSE_LENGTH])

def apply_cuteness(text):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–∏–ª—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫"""
    replacements = {'—Å–ø–∞—Å–∏–±–æ': '—Å–ø—Å', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞': '–ø–∂–ª—Å—Ç–∞', '–ø—Ä–∏–≤–µ—Ç': '–ø—Ä–∏–≤–µ—Ç–∏–∫'}
    for k, v in replacements.items():
        text = text.replace(k, v)
    return text

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ü–µ–Ω–∑—É—Ä–æ–π"""
    user = update.effective_user
    message = update.message.text[:500]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—É—é –ª–µ–∫—Å–∏–∫—É
    if contains_profanity(message):
        await update.message.reply_text("–ú–Ω–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—Ä–µ—Ç–∏–ª –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ç–∞–∫–∏–µ —Å–ª–æ–≤–∞! (‚óïÔ∏µ‚óï)")
        return
    
    if not any(name in message.lower() for name in ["—Å—É–º–∏", "—Å—É–º–∏–∫–æ"]):
        return
    
    user_data = context.user_data.setdefault('sumi', {
        'history': [],
        'train_buffer': [],
        'last_trained': None
    })
    
    response = generate_response(message, user_data['history'])
    user_data['history'].append((message, response))
    user_data['train_buffer'].append((message, response))
    
    user_data['history'] = user_data['history'][-10:]
    
    if len(user_data['train_buffer']) >= 5:
        await schedule_training(user_data)
    
    await update.message.reply_text(response)

async def schedule_training(user_data):
    """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    buffer = user_data['train_buffer'].copy()
    user_data['train_buffer'].clear()
    
    valid_data = [
        (q, a) for q, a in buffer 
        if len(a) > 15 
        and any(e in a for e in ['‚ô™', '‚âßœâ‚â¶'])
        and not contains_profanity(q)
        and not contains_profanity(a)
    ]
    
    if valid_data:
        try:
            await asyncio.to_thread(fine_tune_model, valid_data)
            logger.info(f"Trained on {len(valid_data)} examples")
        except Exception as e:
            logger.error(f"Training error: {str(e)}")

def fine_tune_model(data):
    """–î–æ–æ–±—É—á–µ–Ω–∏–µ —Å LoRA"""
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    lora_model = get_peft_model(model, lora_config)
    
    dataset = Dataset.from_dict({
        "text": [format_prompt(q, []) + a for q, a in data]
    })
    dataset = dataset.map(
        lambda x: tokenizer(x["text"], truncation=True, max_length=512), 
        batched=True
    )
    
    training_args = TrainingArguments(
        output_dir="./sumi-lora",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        learning_rate=1e-4,
        fp16=torch.cuda.is_available(),
        logging_steps=1
    )
    
    Trainer(
        model=lora_model,
        args=training_args,
        train_dataset=dataset,
    ).train()

def main():
    application = Application.builder().token(BOT_TOKEN).build()
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_handler(CommandHandler("reset", lambda u,c: c.user_data.clear()))
    application.run_polling()

if __name__ == "__main__":
    main()
